types:
  AssistantEnd:
    docs: When provided, the output is an assistant end message.
    properties:
      custom_session_id:
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
        type: optional<string>
      type:
        docs: >-
          The type of message sent through the socket; for an Assistant End
          message, this must be `assistant_end`.


          This message indicates the conclusion of the assistant’s response,
          signaling that the assistant has finished speaking for the current
          conversational turn.
        type: literal<"assistant_end">
    source:
      openapi: assistant-asyncapi.json
  AssistantInput:
    docs: When provided, the input is spoken by EVI.
    properties:
      custom_session_id:
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
        type: optional<string>
      text:
        docs: >-
          Assistant text to synthesize into spoken audio and insert into the
          conversation.


          EVI uses this text to generate spoken audio using our proprietary
          expressive text-to-speech model. Our model adds appropriate emotional
          inflections and tones to the text based on the user’s expressions and
          the context of the conversation. The synthesized audio is streamed
          back to the user as an [Assistant
          Message](/reference/empathic-voice-interface-evi/chat/chat#receive.Assistant%20Message.type).
        type: string
      type:
        docs: >-
          The type of message sent through the socket; must be `assistant_input`
          for our server to correctly identify and process it as an Assistant
          Input message.
        type: literal<"assistant_input">
    source:
      openapi: assistant-asyncapi.json
  AssistantMessage:
    docs: When provided, the output is an assistant message.
    properties:
      custom_session_id:
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
        type: optional<string>
      from_text:
        docs: >-
          Indicates if this message was inserted into the conversation as text
          from an [Assistant Input
          message](/reference/empathic-voice-interface-evi/chat/chat#send.Assistant%20Input.text).
        type: boolean
      id:
        docs: >-
          ID of the assistant message. Allows the Assistant Message to be
          tracked and referenced.
        type: optional<string>
      message:
        docs: Transcript of the message.
        type: ChatMessage
      models:
        docs: Inference model results.
        type: Inference
      type:
        docs: >-
          The type of message sent through the socket; for an Assistant Message,
          this must be `assistant_message`.


          This message contains both a transcript of the assistant’s response
          and the expression measurement predictions of the assistant’s audio
          output.
        type: literal<"assistant_message">
    source:
      openapi: assistant-asyncapi.json
  AudioConfiguration:
    properties:
      channels:
        docs: Number of audio channels.
        type: integer
      encoding:
        docs: Encoding format of the audio input, such as `linear16`.
        type: Encoding
      sample_rate:
        docs: >-
          Audio sample rate. Number of samples per second in the audio input,
          measured in Hertz.
        type: integer
    source:
      openapi: assistant-asyncapi.json
  AudioInput:
    docs: When provided, the input is audio.
    properties:
      custom_session_id:
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
        type: optional<string>
      data:
        docs: >-
          Base64 encoded audio input to insert into the conversation.


          The content of an Audio Input message is treated as the user’s speech
          to EVI and must be streamed continuously. Pre-recorded audio files are
          not supported.


          For optimal transcription quality, the audio data should be
          transmitted in small chunks.


          Hume recommends streaming audio with a buffer window of 20
          milliseconds (ms), or 100 milliseconds (ms) for web applications.
        type: string
      type:
        docs: >-
          The type of message sent through the socket; must be `audio_input` for
          our server to correctly identify and process it as an Audio Input
          message.


          This message is used for sending audio input data to EVI for
          processing and expression measurement. Audio data should be sent as a
          continuous stream, encoded in Base64.
        type: literal<"audio_input">
    source:
      openapi: assistant-asyncapi.json
  AudioOutput:
    docs: >-
      The type of message sent through the socket; for an Audio Output message,
      this must be `audio_output`.
    properties:
      custom_session_id:
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
        type: optional<string>
      data:
        docs: >-
          Base64 encoded audio output. This encoded audio is transmitted to the
          client, where it can be decoded and played back as part of the user
          interaction.
        type: string
      id:
        docs: >-
          ID of the audio output. Allows the Audio Output message to be tracked
          and referenced.
        type: string
      index:
        docs: Index of the chunk of audio relative to the whole audio segment.
        type: integer
      type:
        docs: >-
          The type of message sent through the socket; for an Audio Output
          message, this must be `audio_output`.
        type: literal<"audio_output">
    source:
      openapi: assistant-asyncapi.json
  BuiltInTool:
    docs: >-
      Name of the built-in tool. Set to `web_search` to equip EVI with the
      built-in Web Search tool.
    enum:
      - web_search
      - hang_up
    source:
      openapi: assistant-asyncapi.json
  BuiltinToolConfig:
    properties:
      fallback_content:
        docs: >-
          Optional text passed to the supplemental LLM if the tool call fails.
          The LLM then uses this text to generate a response back to the user,
          ensuring continuity in the conversation.
        type: optional<string>
      name:
        type: BuiltInTool
    source:
      openapi: assistant-asyncapi.json
  ChatMessage:
    properties:
      content:
        docs: Transcript of the message.
        type: optional<string>
      role:
        docs: Role of who is providing the message.
        type: Role
      tool_call:
        docs: Function call name and arguments.
        type: optional<ToolCallMessage>
      tool_result:
        docs: Function call response from client.
        type: optional<ChatMessageToolResult>
    source:
      openapi: assistant-asyncapi.json
  ChatMessageToolResult:
    discriminated: false
    docs: Function call response from client.
    inline: true
    source:
      openapi: assistant-asyncapi.json
    union:
      - type: ToolResponseMessage
      - type: ToolErrorMessage
  ChatMetadata:
    docs: When provided, the output is a chat metadata message.
    properties:
      chat_group_id:
        docs: >-
          ID of the Chat Group.


          Used to resume a Chat when passed in the
          [resumed_chat_group_id](/reference/empathic-voice-interface-evi/chat/chat#request.query.resumed_chat_group_id)
          query parameter of a subsequent connection request. This allows EVI to
          continue the conversation from where it left off within the Chat
          Group.


          Learn more about [supporting chat
          resumability](/docs/empathic-voice-interface-evi/faq#does-evi-support-chat-resumability)
          from the EVI FAQ.
        type: string
      chat_id:
        docs: >-
          ID of the Chat session. Allows the Chat session to be tracked and
          referenced.
        type: string
      custom_session_id:
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
        type: optional<string>
      request_id:
        docs: ID of the initiating request.
        type: optional<string>
      type:
        docs: >-
          The type of message sent through the socket; for a Chat Metadata
          message, this must be `chat_metadata`.


          The Chat Metadata message is the first message you receive after
          establishing a connection with EVI and contains important identifiers
          for the current Chat session.
        type: literal<"chat_metadata">
    source:
      openapi: assistant-asyncapi.json
  Context:
    properties:
      text:
        docs: >-
          The context to be injected into the conversation. Helps inform the
          LLM's response by providing relevant information about the ongoing
          conversation.


          This text will be appended to the end of user messages based on the
          chosen persistence level. For example, if you want to remind EVI of
          its role as a helpful weather assistant, the context you insert will
          be appended to the end of user messages as `{Context: You are a
          helpful weather assistant}`.
        type: string
      type:
        docs: >-
          The persistence level of the injected context. Specifies how long the
          injected context will remain active in the session.


          There are three possible context types:


          - **Persistent**: The context is appended to all user messages for the
          duration of the session.


          - **Temporary**: The context is appended only to the next user
          message.

           - **Editable**: The original context is updated to reflect the new context.

           If the type is not specified, it will default to `temporary`.
        type: optional<ContextType>
    source:
      openapi: assistant-asyncapi.json
  ContextType:
    enum:
      - editable
      - persistent
      - temporary
    source:
      openapi: assistant-asyncapi.json
  EmotionScores:
    properties:
      Admiration: double
      Adoration: double
      Aesthetic Appreciation: double
      Amusement: double
      Anger: double
      Anxiety: double
      Awe: double
      Awkwardness: double
      Boredom: double
      Calmness: double
      Concentration: double
      Confusion: double
      Contemplation: double
      Contempt: double
      Contentment: double
      Craving: double
      Desire: double
      Determination: double
      Disappointment: double
      Disgust: double
      Distress: double
      Doubt: double
      Ecstasy: double
      Embarrassment: double
      Empathic Pain: double
      Entrancement: double
      Envy: double
      Excitement: double
      Fear: double
      Guilt: double
      Horror: double
      Interest: double
      Joy: double
      Love: double
      Nostalgia: double
      Pain: double
      Pride: double
      Realization: double
      Relief: double
      Romance: double
      Sadness: double
      Satisfaction: double
      Shame: double
      Surprise (negative): double
      Surprise (positive): double
      Sympathy: double
      Tiredness: double
      Triumph: double
    source:
      openapi: assistant-asyncapi.json
  Encoding:
    type: literal<"linear16">
  ErrorLevel:
    type: literal<"warn">
  FunctionCallResponseInput:
    properties:
      type: optional<literal<"function_call_response">>
    source:
      openapi: assistant-asyncapi.json
  Inference:
    properties:
      prosody:
        docs: >-
          Prosody model inference results.


          EVI uses the prosody model to measure 48 emotions related to speech
          and vocal characteristics within a given expression.
        type: optional<ProsodyInference>
    source:
      openapi: assistant-asyncapi.json
  JsonMessage:
    discriminated: false
    source:
      openapi: assistant-asyncapi.json
    union:
      - type: AssistantEnd
      - type: AssistantMessage
      - type: ChatMetadata
      - type: WebSocketError
      - type: UserInterruption
      - type: UserMessage
      - type: ToolCallMessage
      - type: ToolResponseMessage
      - type: ToolErrorMessage
  MillisecondInterval:
    properties:
      begin:
        docs: Start time of the interval in milliseconds.
        type: integer
      end:
        docs: End time of the interval in milliseconds.
        type: integer
    source:
      openapi: assistant-asyncapi.json
  PauseAssistantMessage:
    docs: >-
      Pause responses from EVI. Chat history is still saved and sent after
      resuming. 
    properties:
      custom_session_id:
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
        type: optional<string>
      type:
        docs: >-
          The type of message sent through the socket; must be
          `pause_assistant_message` for our server to correctly identify and
          process it as a Pause Assistant message.


          Once this message is sent, EVI will not respond until a [Resume
          Assistant
          message](/reference/empathic-voice-interface-evi/chat/chat#send.Resume%20Assistant%20Message.type)
          is sent. When paused, EVI won’t respond, but transcriptions of your
          audio inputs will still be recorded.
        type: literal<"pause_assistant_message">
    source:
      openapi: assistant-asyncapi.json
  ProsodyInference:
    properties:
      scores:
        docs: >-
          The confidence scores for 48 emotions within the detected expression
          of an audio sample.


          Scores typically range from 0 to 1, with higher values indicating a
          stronger confidence level in the measured attribute.


          See our guide on [interpreting expression measurement
          results](/docs/expression-measurement/faq#how-do-i-interpret-my-results)
          to learn more.
        type: EmotionScores
    source:
      openapi: assistant-asyncapi.json
  ResumeAssistantMessage:
    docs: >-
      Resume responses from EVI. Chat history sent while paused will now be
      sent. 
    properties:
      custom_session_id:
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
        type: optional<string>
      type:
        docs: >-
          The type of message sent through the socket; must be
          `resume_assistant_message` for our server to correctly identify and
          process it as a Resume Assistant message.


          Upon resuming, if any audio input was sent during the pause, EVI will
          retain context from all messages sent but only respond to the last
          user message. (e.g., If you ask EVI two questions while paused and
          then send a `resume_assistant_message`, EVI will respond to the second
          question and have added the first question to its conversation
          context.)
        type: literal<"resume_assistant_message">
    source:
      openapi: assistant-asyncapi.json
  Role:
    enum:
      - assistant
      - system
      - user
      - all
      - tool
    source:
      openapi: assistant-asyncapi.json
  SessionSettings:
    docs: Settings for this chat session.
    properties:
      audio:
        docs: >-
          Configuration details for the audio input used during the session.
          Ensures the audio is being correctly set up for processing.


          This optional field is only required when the audio input is encoded
          in PCM Linear 16 (16-bit, little-endian, signed PCM WAV data). For
          detailed instructions on how to configure session settings for PCM
          Linear 16 audio, please refer to the [Session Settings
          section](/docs/empathic-voice-interface-evi/configuration#session-settings)
          on the EVI Configuration page.
        type: optional<AudioConfiguration>
      builtin_tools:
        docs: >-
          List of built-in tools to enable for the session.


          Tools are resources used by EVI to perform various tasks, such as
          searching the web or calling external APIs. Built-in tools, like web
          search, are natively integrated, while user-defined tools are created
          and invoked by the user. To learn more, see our [Tool Use
          Guide](/docs/empathic-voice-interface-evi/tool-use).


          Currently, the only built-in tool Hume provides is **Web Search**.
          When enabled, Web Search equips EVI with the ability to search the web
          for up-to-date information.
        type: optional<list<BuiltinToolConfig>>
      context:
        docs: >-
          Allows developers to inject additional context into the conversation,
          which is appended to the end of user messages for the session.


          When included in a Session Settings message, the provided context can
          be used to remind the LLM of its role in every user message, prevent
          it from forgetting important details, or add new relevant information
          to the conversation.


          Set to `null` to disable context injection.
        type: optional<Context>
      custom_session_id:
        docs: >-
          Unique identifier for the session. Used to manage conversational
          state, correlate frontend and backend data, and persist conversations
          across EVI sessions.


          If included, the response sent from Hume to your backend will include
          this ID. This allows you to correlate frontend users with their
          incoming messages.


          It is recommended to pass a `custom_session_id` if you are using a
          Custom Language Model. Please see our guide to [using a custom
          language
          model](/docs/empathic-voice-interface-evi/custom-language-model) with
          EVI to learn more.
        type: optional<string>
      language_model_api_key:
        docs: >-
          Third party API key for the supplemental language model.


          When provided, EVI will use this key instead of Hume’s API key for the
          supplemental LLM. This allows you to bypass rate limits and utilize
          your own API key as needed.
        type: optional<string>
      metadata:
        type: optional<map<string, unknown>>
      system_prompt:
        docs: >-
          Instructions used to shape EVI’s behavior, responses, and style for
          the session.


          When included in a Session Settings message, the provided Prompt
          overrides the existing one specified in the EVI configuration. If no
          Prompt was defined in the configuration, this Prompt will be the one
          used for the session.


          You can use the Prompt to define a specific goal or role for EVI,
          specifying how it should act or what it should focus on during the
          conversation. For example, EVI can be instructed to act as a customer
          support representative, a fitness coach, or a travel advisor, each
          with its own set of behaviors and response styles.


          For help writing a system prompt, see our [Prompting
          Guide](/docs/empathic-voice-interface-evi/prompting).
        type: optional<string>
      tools:
        docs: >-
          List of user-defined tools to enable for the session.


          Tools are resources used by EVI to perform various tasks, such as
          searching the web or calling external APIs. Built-in tools, like web
          search, are natively integrated, while user-defined tools are created
          and invoked by the user. To learn more, see our [Tool Use
          Guide](/docs/empathic-voice-interface-evi/tool-use).
        type: optional<list<Tool>>
      type:
        docs: >-
          The type of message sent through the socket; must be
          `session_settings` for our server to correctly identify and process it
          as a Session Settings message.


          Session settings are temporary and apply only to the current Chat
          session. These settings can be adjusted dynamically based on the
          requirements of each session to ensure optimal performance and user
          experience.


          For more information, please refer to the [Session Settings
          section](/docs/empathic-voice-interface-evi/configuration#session-settings)
          on the EVI Configuration page.
        type: literal<"session_settings">
      variables:
        docs: >-
          This field allows you to assign values to dynamic variables referenced
          in your system prompt.


          Each key represents the variable name, and the corresponding value is
          the specific content you wish to assign to that variable within the
          session. While the values for variables can be strings, numbers, or
          booleans, the value will ultimately be converted to a string when
          injected into your system prompt.


          Using this field, you can personalize responses based on
          session-specific details. For more guidance, see our [guide on using
          dynamic
          variables](/docs/empathic-voice-interface-evi/conversational-controls#dynamic-variables).
        type: optional<map<string, SessionSettingsVariablesValue>>
    source:
      openapi: assistant-asyncapi.json
  SessionSettingsVariablesValue:
    discriminated: false
    inline: true
    source:
      openapi: assistant-asyncapi.json
    union:
      - string
      - double
      - boolean
  TextInput:
    properties:
      type: optional<literal<"text_input">>
    source:
      openapi: assistant-asyncapi.json
  Tool:
    properties:
      description:
        docs: >-
          An optional description of what the tool does, used by the
          supplemental LLM to choose when and how to call the function.
        type: optional<string>
      fallback_content:
        docs: >-
          Optional text passed to the supplemental LLM if the tool call fails.
          The LLM then uses this text to generate a response back to the user,
          ensuring continuity in the conversation.
        type: optional<string>
      name:
        docs: Name of the user-defined tool to be enabled.
        type: string
      parameters:
        docs: >-
          Parameters of the tool. Is a stringified JSON schema.


          These parameters define the inputs needed for the tool’s execution,
          including the expected data type and description for each input field.
          Structured as a JSON schema, this format ensures the tool receives
          data in the expected format.
        type: string
      type:
        docs: Type of tool. Set to `function` for user-defined tools.
        type: ToolType
    source:
      openapi: assistant-asyncapi.json
  ToolCallMessage:
    docs: When provided, the output is a tool call.
    properties:
      custom_session_id:
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
        type: optional<string>
      name:
        docs: Name of the tool called.
        type: string
      parameters:
        docs: >-
          Parameters of the tool.


          These parameters define the inputs needed for the tool’s execution,
          including the expected data type and description for each input field.
          Structured as a stringified JSON schema, this format ensures the tool
          receives data in the expected format.
        type: string
      response_required:
        docs: >-
          Indicates whether a response to the tool call is required from the
          developer, either in the form of a [Tool Response
          message](/reference/empathic-voice-interface-evi/chat/chat#send.Tool%20Response%20Message.type)
          or a [Tool Error
          message](/reference/empathic-voice-interface-evi/chat/chat#send.Tool%20Error%20Message.type).
        type: boolean
      tool_call_id:
        docs: >-
          The unique identifier for a specific tool call instance.


          This ID is used to track the request and response of a particular tool
          invocation, ensuring that the correct response is linked to the
          appropriate request.
        type: string
      tool_type:
        docs: >-
          Type of tool called. Either `builtin` for natively implemented tools,
          like web search, or `function` for user-defined tools.
        type: optional<ToolType>
      type:
        docs: >-
          The type of message sent through the socket; for a Tool Call message,
          this must be `tool_call`.


          This message indicates that the supplemental LLM has detected a need
          to invoke the specified tool.
        type: literal<"tool_call">
    source:
      openapi: assistant-asyncapi.json
  ToolErrorMessage:
    docs: When provided, the output is a function call error.
    properties:
      code:
        docs: Error code. Identifies the type of error encountered.
        type: optional<string>
      content:
        docs: >-
          Optional text passed to the supplemental LLM in place of the tool call
          result. The LLM then uses this text to generate a response back to the
          user, ensuring continuity in the conversation if the tool errors.
        type: optional<string>
      custom_session_id:
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
        type: optional<string>
      error:
        docs: Error message from the tool call, not exposed to the LLM or user.
        type: string
      level:
        docs: >-
          Indicates the severity of an error; for a Tool Error message, this
          must be `warn` to signal an unexpected event.
        type: optional<ErrorLevel>
      tool_call_id:
        docs: >-
          The unique identifier for a specific tool call instance.


          This ID is used to track the request and response of a particular tool
          invocation, ensuring that the Tool Error message is linked to the
          appropriate tool call request. The specified `tool_call_id` must match
          the one received in the [Tool Call
          message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type).
        type: string
      tool_type:
        docs: >-
          Type of tool called. Either `builtin` for natively implemented tools,
          like web search, or `function` for user-defined tools.
        type: optional<ToolType>
      type:
        docs: >-
          The type of message sent through the socket; for a Tool Error message,
          this must be `tool_error`.


          Upon receiving a [Tool Call
          message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type)
          and failing to invoke the function, this message is sent to notify EVI
          of the tool's failure.
        type: literal<"tool_error">
    source:
      openapi: assistant-asyncapi.json
  ToolResponseMessage:
    docs: When provided, the output is a function call response.
    properties:
      content:
        docs: >-
          Return value of the tool call. Contains the output generated by the
          tool to pass back to EVI.
        type: string
      custom_session_id:
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
        type: optional<string>
      tool_call_id:
        docs: >-
          The unique identifier for a specific tool call instance.


          This ID is used to track the request and response of a particular tool
          invocation, ensuring that the correct response is linked to the
          appropriate request. The specified `tool_call_id` must match the one
          received in the [Tool Call
          message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.tool_call_id).
        type: string
      tool_name:
        docs: >-
          Name of the tool.


          Include this optional field to help the supplemental LLM identify
          which tool generated the response. The specified `tool_name` must
          match the one received in the [Tool Call
          message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type).
        type: optional<string>
      tool_type:
        docs: >-
          Type of tool called. Either `builtin` for natively implemented tools,
          like web search, or `function` for user-defined tools.
        type: optional<ToolType>
      type:
        docs: >-
          The type of message sent through the socket; for a Tool Response
          message, this must be `tool_response`.


          Upon receiving a [Tool Call
          message](/reference/empathic-voice-interface-evi/chat/chat#receive.Tool%20Call%20Message.type)
          and successfully invoking the function, this message is sent to convey
          the result of the function call back to EVI.
        type: literal<"tool_response">
    source:
      openapi: assistant-asyncapi.json
  ToolType:
    enum:
      - builtin
      - function
    source:
      openapi: assistant-asyncapi.json
  TtsInput:
    properties:
      type: optional<literal<"tts">>
    source:
      openapi: assistant-asyncapi.json
  UserInput:
    docs: >-
      User text to insert into the conversation. Text sent through a User Input
      message is treated as the user’s speech to EVI. EVI processes this input
      and provides a corresponding response.


      Expression measurement results are not available for User Input messages,
      as the prosody model relies on audio input and cannot process text alone.
    properties:
      custom_session_id:
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
        type: optional<string>
      text:
        docs: >-
          User text to insert into the conversation. Text sent through a User
          Input message is treated as the user’s speech to EVI. EVI processes
          this input and provides a corresponding response.


          Expression measurement results are not available for User Input
          messages, as the prosody model relies on audio input and cannot
          process text alone.
        type: string
      type:
        docs: >-
          The type of message sent through the socket; must be `user_input` for
          our server to correctly identify and process it as a User Input
          message.
        type: literal<"user_input">
    source:
      openapi: assistant-asyncapi.json
  UserInterruption:
    docs: When provided, the output is an interruption.
    properties:
      custom_session_id:
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
        type: optional<string>
      time:
        docs: Unix timestamp of the detected user interruption.
        type: integer
      type:
        docs: >-
          The type of message sent through the socket; for a User Interruption
          message, this must be `user_interruption`.


          This message indicates the user has interrupted the assistant’s
          response. EVI detects the interruption in real-time and sends this
          message to signal the interruption event. This message allows the
          system to stop the current audio playback, clear the audio queue, and
          prepare to handle new user input.
        type: literal<"user_interruption">
    source:
      openapi: assistant-asyncapi.json
  UserMessage:
    docs: When provided, the output is a user message.
    properties:
      custom_session_id:
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
        type: optional<string>
      from_text:
        docs: >-
          Indicates if this message was inserted into the conversation as text
          from a [User
          Input](/reference/empathic-voice-interface-evi/chat/chat#send.User%20Input.text)
          message.
        type: boolean
      interim:
        docs: >-
          Indicates if this message contains an immediate and unfinalized
          transcript of the user’s audio input. If it does, words may be
          repeated across successive `UserMessage` messages as our transcription
          model becomes more confident about what was said with additional
          context. Interim messages are useful to detect if the user is
          interrupting during audio playback on the client. Even without a
          finalized transcription, along with
          [UserInterrupt](/reference/empathic-voice-interface-evi/chat/chat#receive.User%20Interruption.type)
          messages, interim `UserMessages` are useful for detecting if the user
          is interrupting during audio playback on the client, signaling to stop
          playback in your application. Interim `UserMessages` will only be
          received if the
          [verbose_transcription](/reference/empathic-voice-interface-evi/chat/chat#request.query.verbose_transcription)
          query parameter is set to `true` in the handshake request.
        type: boolean
      message:
        docs: Transcript of the message.
        type: ChatMessage
      models:
        docs: Inference model results.
        type: Inference
      time:
        docs: Start and End time of user message.
        type: MillisecondInterval
      type:
        docs: >-
          The type of message sent through the socket; for a User Message, this
          must be `user_message`.


          This message contains both a transcript of the user’s input and the
          expression measurement predictions if the input was sent as an [Audio
          Input
          message](/reference/empathic-voice-interface-evi/chat/chat#send.Audio%20Input.type).
          Expression measurement predictions are not provided for a [User Input
          message](/reference/empathic-voice-interface-evi/chat/chat#send.User%20Input.type),
          as the prosody model relies on audio input and cannot process text
          alone.
        type: literal<"user_message">
    source:
      openapi: assistant-asyncapi.json
  WebSocketError:
    docs: When provided, the output is an error message.
    properties:
      code:
        docs: Error code. Identifies the type of error encountered.
        type: string
      custom_session_id:
        docs: >-
          Used to manage conversational state, correlate frontend and backend
          data, and persist conversations across EVI sessions.
        type: optional<string>
      message:
        docs: Detailed description of the error.
        type: string
      slug:
        docs: >-
          Short, human-readable identifier and description for the error. See a
          complete list of error slugs on the [Errors
          page](/docs/resources/errors).
        type: string
      type:
        docs: >-
          The type of message sent through the socket; for a Web Socket Error
          message, this must be `error`.


          This message indicates a disruption in the WebSocket connection, such
          as an unexpected disconnection, protocol error, or data transmission
          issue.
        type: literal<"error">
    source:
      openapi: assistant-asyncapi.json
